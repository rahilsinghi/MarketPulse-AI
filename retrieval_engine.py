"""
MarketPulse AI - Real-Time Retrieval Engine
Implements the 5 core functions as specified.
"""

import os
import time
import asyncio
import logging
from typing import List, Dict, Optional
from functools import lru_cache
import numpy as np

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MOCK_EMBEDDING = os.getenv("MOCK_EMBEDDING", "false").lower() == "true"  # Default to mock for demo

# Optional imports - will be loaded when needed
openai = None
faiss = None

def _lazy_import_openai():
    """Lazy import OpenAI to avoid top-level network calls."""
    global openai
    if openai is None and not MOCK_EMBEDDING:
        try:
            import openai as _openai
            openai = _openai
            logger.info("OpenAI module loaded successfully")
        except ImportError:
            logger.warning("OpenAI not available, using mock mode")
            openai = None
    return openai

def _lazy_import_faiss():
    """Lazy import FAISS for vector search."""
    global faiss
    if faiss is None:
        try:
            import faiss as _faiss
            faiss = _faiss
            logger.info("FAISS module loaded successfully")
        except ImportError:
            logger.info("FAISS not available, falling back to linear search")
            faiss = None
    return faiss

# 1Ô∏è‚É£ EMBEDDING FUNCTION
@lru_cache(maxsize=256)
def embed_text(text: str) -> List[float]:
    """
    Call OpenAI text-embedding-3-small (1536-dim) unless MOCK_EMBEDDING=true.
    Uses @lru_cache(maxsize=256) to avoid duplicate API calls.
    """
    if not text or not text.strip():
        logger.warning("Empty text provided for embedding")
        return [0.0] * 1536
    
    # Always use mock for demo unless explicitly configured otherwise
    if MOCK_EMBEDDING or not OPENAI_API_KEY:
        # Deterministic mock embedding
        np.random.seed(hash(text) % 2**32)
        embedding = np.random.rand(1536).astype(float).tolist()
        logger.debug(f"Generated mock embedding for: {text[:50]}...")
        return embedding
    
    try:
        # Only try OpenAI if explicitly enabled and configured
        from openai import OpenAI
        
        client = OpenAI(
            api_key=OPENAI_API_KEY,
            # Remove any proxy or other problematic parameters
        )
        
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=[text.strip()[:8000]]  # Truncate if too long
        )
        
        embedding = response.data[0].embedding
        logger.debug(f"Generated OpenAI embedding for: {text[:50]}...")
        return embedding
        
    except Exception as e:
        logger.warning(f"OpenAI embedding failed, using mock: {e}")
        # Fallback to mock embedding
        np.random.seed(hash(text) % 2**32)
        return np.random.rand(1536).astype(float).tolist()

# 2Ô∏è‚É£ RETRIEVAL FUNCTION
def retrieve_top_k(query_vec: List[float], k: int = 4) -> List[dict]:
    """
    Pull live rows from ingest_pipeline.get_latest_rows().
    If rows > 1000 ‚Üí build/reuse a FAISS IndexFlatIP for O(log n) search.
    Otherwise fall back to cosine loop.
    """
    try:
        # Import the ingest pipeline
        from ingest_pipeline import get_latest_rows
        rows = get_latest_rows()
    except ImportError:
        logger.warning("ingest_pipeline not available, using mock data")
        rows = _get_mock_rows()
    
    if not rows:
        logger.warning("No rows available for retrieval")
        return []
    
    # Filter rows that have embeddings
    valid_rows = [row for row in rows if row.get('embedding')]
    
    if len(valid_rows) > 1000:
        # Use FAISS for large datasets
        return _faiss_search(query_vec, valid_rows, k)
    else:
        # Use linear search for smaller datasets
        return _linear_search(query_vec, valid_rows, k)

def _faiss_search(query_vec: List[float], rows: List[dict], k: int) -> List[dict]:
    """Use FAISS IndexFlatIP for efficient search."""
    faiss_lib = _lazy_import_faiss()
    if not faiss_lib:
        logger.info("FAISS not available, using linear search")
        return _linear_search(query_vec, rows, k)
    
    try:
        # Build FAISS index
        embeddings = np.array([row['embedding'] for row in rows], dtype=np.float32)
        
        # Normalize vectors for inner product similarity
        faiss_lib.normalize_L2(embeddings)
        query_norm = np.array([query_vec], dtype=np.float32)
        faiss_lib.normalize_L2(query_norm)
        
        # Create index
        index = faiss_lib.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings)
        
        # Search
        similarities, indices = index.search(query_norm, min(k, len(rows)))
        
        # Build results
        results = []
        for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
            if idx < len(rows):
                result = rows[idx].copy()
                result['similarity'] = float(similarity)
                results.append(result)
        
        logger.info(f"FAISS search returned {len(results)} results")
        return results
        
    except Exception as e:
        logger.error(f"FAISS search failed: {e}, falling back to linear search")
        return _linear_search(query_vec, rows, k)

def _linear_search(query_vec: List[float], rows: List[dict], k: int) -> List[dict]:
    """Linear search with cosine similarity."""
    scored_docs = []
    query_vec = np.array(query_vec)
    
    for row in rows:
        try:
            doc_vec = np.array(row['embedding'])
            similarity = cosine_similarity(query_vec, doc_vec)
            
            result = row.copy()
            result['similarity'] = float(similarity)
            scored_docs.append(result)
            
        except Exception as e:
            logger.warning(f"Error calculating similarity for row: {e}")
            continue
    
    # Sort by similarity and return top-k
    scored_docs.sort(key=lambda x: x['similarity'], reverse=True)
    results = scored_docs[:k]
    
    logger.info(f"Linear search returned {len(results)} results")
    return results

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """Calculate cosine similarity between two vectors."""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# 3Ô∏è‚É£ PROMPT BUILDING FUNCTION
def build_prompt(question: str, docs: List[dict]) -> List[dict]:
    """
    Build messages for GPT-4o with system prompt and formatted context.
    """
    system_msg = (
        "You are a concise, real-time market analyst. Answer questions about "
        "financial news in ‚â§3 sentences. Always cite relevant information by "
        "ticker symbol. Be direct and factual."
    )
    
    if not docs:
        context = "No recent relevant news available."
    else:
        context_parts = []
        for doc in docs:
            ticker = doc.get('ticker', 'UNKNOWN')
            headline = doc.get('headline', 'No headline')
            timestamp = doc.get('timestamp', '')
            
            # Format timestamp if available
            timestamp_str = ""
            if timestamp:
                try:
                    from datetime import datetime
                    if isinstance(timestamp, str):
                        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                        timestamp_str = f" ‚Äî {dt.strftime('%Y-%m-%d %H:%M')}"
                except:
                    timestamp_str = f" ‚Äî {timestamp}"
            
            context_parts.append(f"‚Ä¢ [{ticker}] {headline}{timestamp_str}")
        
        context = "\n".join(context_parts)
    
    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": f"Recent news:\n{context}\n\nQuestion: {question}"}
    ]
    
    return messages

# 4Ô∏è‚É£ SYNCHRONOUS QUERY FUNCTION
def answer_query_sync(question: str, top_k: int = 4) -> str:
    """
    Time the full path and return "(üïí 0.8 s) <answer text>".
    If no docs with similarity > 0.25 ‚Üí reply "No fresh information yet."
    """
    t0 = time.time()
    
    try:
        # Get query embedding
        query_vec = embed_text(question)
        
        # Retrieve relevant documents
        docs = retrieve_top_k(query_vec, top_k)
        
        # DEBUG: Log what we found
        logger.info(f"Query: '{question}' found {len(docs)} documents")
        for i, doc in enumerate(docs[:3]):
            ticker = doc.get('ticker', 'N/A')
            similarity = doc.get('similarity', 0)
            headline = doc.get('headline', 'N/A')[:60]
            logger.info(f"  {i+1}. [{ticker}] {similarity:.3f} - {headline}...")
        
        # Filter by similarity threshold
        relevant_docs = [doc for doc in docs if doc.get('similarity', 0) > 0.25]
        
        if not relevant_docs:
            t1 = time.time()
            return f"(üïí {t1-t0:.1f}s) No fresh information yet."
        
        # Build prompt
        messages = build_prompt(question, relevant_docs)
        
        # DEBUG: Log our mode and what we're trying
        logger.info(f"MOCK_EMBEDDING: {MOCK_EMBEDDING}, OPENAI_API_KEY: {'SET' if OPENAI_API_KEY else 'NOT SET'}")
        
        # Try real OpenAI chat completion FIRST
        if OPENAI_API_KEY and not MOCK_EMBEDDING:
            try:
                logger.info("ü§ñ Attempting real OpenAI chat completion...")
                answer = asyncio.run(_answer_query_async(messages))
                logger.info("‚úÖ Real OpenAI chat response generated successfully")
                t1 = time.time()
                return f"(üïí {t1-t0:.1f}s) {answer}"
            except Exception as e:
                logger.warning(f"‚ùå OpenAI chat failed: {e}")
                logger.info("üîÑ Falling back to smart mock response...")
        else:
            logger.info("üß™ Using mock mode (MOCK_EMBEDDING=true or no API key)")
        
        # Fallback to smart mock based on retrieved documents
        answer = _generate_smart_mock_response(messages, relevant_docs, question)
        t1 = time.time()
        return f"(üïí {t1-t0:.1f}s) {answer}"
        
    except Exception as e:
        t1 = time.time()
        logger.error(f"Error in answer_query_sync: {e}")
        return f"(üïí {t1-t0:.1f}s) Error processing query: {str(e)}"

# 5Ô∏è‚É£ ASYNCHRONOUS ANSWER FUNCTION
async def _answer_query_async(messages: List[dict]) -> str:
    """
    Use openai.ChatCompletion.acreate(model="gpt-4o-mini").
    Handle rate limit errors with backoff and retry.
    """
    # For demo, always use mock response to avoid API issues
    if MOCK_EMBEDDING or not OPENAI_API_KEY:
        return _generate_mock_response(messages)
    
    try:
        from openai import AsyncOpenAI
        
        async_client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            # Remove any problematic parameters
        )
        
        try:
            response = await async_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_tokens=300,
                temperature=0.1
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as rate_error:
            if "rate" in str(rate_error).lower():
                logger.warning("Rate limit hit, waiting 2s and retrying...")
                await asyncio.sleep(2)
                
                # Retry once
                response = await async_client.chat.completions.create(
                    model="gpt-4o-mini", 
                    messages=messages,
                    max_tokens=300,
                    temperature=0.1
                )
                
                return response.choices[0].message.content.strip()
            else:
                raise rate_error
                
    except Exception as e:
        logger.warning(f"OpenAI API failed, using mock response: {e}")
        return _generate_mock_response(messages)

def _generate_smart_mock_response(messages: List[dict], relevant_docs: List[dict], question: str) -> str:
    """
    Generate a smart mock response based on actual retrieved documents.
    This uses the ACTUAL retrieved content, not hardcoded responses.
    """
    if not relevant_docs:
        return "I don't have any recent relevant news for that query."
    
    # Get the most relevant document (first one, highest similarity)
    top_doc = relevant_docs[0]
    ticker = top_doc.get('ticker', 'Unknown')
    headline = top_doc.get('headline', 'No headline available')
    similarity = top_doc.get('similarity', 0)
    source = top_doc.get('source', 'Unknown source')
    
    logger.info(f"üì∞ Generating response based on top result: [{ticker}] {headline}")
    
    # Create a contextual response based on the actual retrieved document
    if similarity > 0.7:
        confidence = "highly relevant"
    elif similarity > 0.5:
        confidence = "relevant"
    else:
        confidence = "potentially relevant"
    
    # Build response using the actual content
    base_response = f"Based on {confidence} recent news, here's what I found about {ticker}: {headline}"
    
    # Add context if we have multiple relevant docs
    if len(relevant_docs) > 1:
        other_docs = relevant_docs[1:3]  # Show up to 2 more
        additional_context = []
        for doc in other_docs:
            if doc.get('ticker') != ticker:  # Different company
                additional_context.append(f"Also noteworthy: [{doc.get('ticker')}] {doc.get('headline', '')[:80]}...")
        
        if additional_context:
            base_response += " " + " ".join(additional_context)
    
    # Add source attribution
    base_response += f" (Source: {source})"
    
    return base_response

def _generate_mock_response(messages: List[dict]) -> str:
    """Generate a mock response when OpenAI is not available."""
    # Extract question from messages
    question = ""
    for msg in messages:
        if msg["role"] == "user":
            question = msg["content"]
            break
    
    # This is now just a fallback - we should use _generate_smart_mock_response instead
    logger.warning("Using basic fallback response - this shouldn't happen with proper retrieval")
    return "I'm having trouble accessing the latest news. Please try your query again."

# UTILITY FUNCTIONS
def _get_mock_rows() -> List[dict]:
    """Fallback mock data when ingest_pipeline is not available."""
    from datetime import datetime, timedelta
    
    base_time = datetime.now()
    mock_data = [
        {
            "ticker": "AAPL",
            "headline": "Apple reports record quarterly earnings, beating analyst expectations",
            "timestamp": (base_time - timedelta(hours=2)).isoformat(),
            "source": "Reuters",
            "embedding": embed_text("Apple reports record quarterly earnings, beating analyst expectations")
        },
        {
            "ticker": "TSLA", 
            "headline": "Tesla delivers 500,000 vehicles in Q4, stock surges 8%",
            "timestamp": (base_time - timedelta(hours=1)).isoformat(),
            "source": "Bloomberg",
            "embedding": embed_text("Tesla delivers 500,000 vehicles in Q4, stock surges 8%")
        },
        {
            "ticker": "GOOGL",
            "headline": "Google announces breakthrough in quantum computing research",
            "timestamp": (base_time - timedelta(minutes=45)).isoformat(),
            "source": "TechCrunch", 
            "embedding": embed_text("Google announces breakthrough in quantum computing research")
        }
    ]
    
    return mock_data

# Thread-safety check
import threading
_lock = threading.Lock()

def thread_safe_embed_text(text: str) -> List[float]:
    """Thread-safe wrapper for embed_text."""
    with _lock:
        return embed_text(text)